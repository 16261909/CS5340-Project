{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-15T09:51:56.729099Z",
     "start_time": "2024-04-15T09:51:56.709549Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "41dd60fbba14db27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T09:51:58.051751Z",
     "start_time": "2024-04-15T09:51:56.730075Z"
    }
   },
   "source": [
    "import os\n",
    "from time import sleep\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import PIL.Image as Image\n",
    "\n",
    "from dataset import *\n",
    "from utilities import *\n",
    "from resnet import *\n",
    "from config import *"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stevenzhang/miniconda3/envs/vnn/lib/python3.10/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "/home/stevenzhang/miniconda3/envs/vnn/lib/python3.10/site-packages/torchvision/transforms/transforms.py:1435: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "8f275ff0803b583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T09:51:58.054845Z",
     "start_time": "2024-04-15T09:51:58.052692Z"
    }
   },
   "source": [
    "color_to_gray_map, gray_to_color_map = None, None"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "503142796444025a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T09:51:58.076007Z",
     "start_time": "2024-04-15T09:51:58.055428Z"
    }
   },
   "source": [
    "np.random.seed(0)\n",
    "train_imageset_path = '../trainval/DAVIS/ImageSets/2017/train.txt'\n",
    "val_imageset_path = '../trainval/DAVIS/ImageSets/2017/val.txt'\n",
    "testd_imageset_path = '../testd/DAVIS/ImageSets/2017/test-dev.txt'\n",
    "trainval_image_root = '../trainval/DAVIS/JPEGImages/480p/'\n",
    "trainval_mask_root = '../trainval/DAVIS/Annotations/480p/'\n",
    "testd_image_root = '../testd/DAVIS/JPEGImages/480p/'\n",
    "testd_mask_root = '../testd/DAVIS/Annotations/480p/'\n",
    "models_root = '../models/'\n",
    "\n",
    "train_list = []\n",
    "val_list = []\n",
    "test_list = []\n",
    "\n",
    "with open(train_imageset_path, 'r') as f:\n",
    "    for line in f:\n",
    "        train_list.append(line.strip())\n",
    "with open(val_imageset_path, 'r') as f:\n",
    "    for line in f:\n",
    "        val_list.append(line.strip())\n",
    "with open(testd_imageset_path, 'r') as f:\n",
    "    for line in f:\n",
    "        test_list.append(line.strip())\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "66f9647f8b7d7358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T09:51:58.091755Z",
     "start_time": "2024-04-15T09:51:58.076949Z"
    }
   },
   "source": [
    "def train(image_root, mask_root, target_list):\n",
    "\n",
    "    for t in range(len(target_list)):\n",
    "        if t != 29:\n",
    "            continue\n",
    "        print(target_list[t])\n",
    "        image_path = os.path.join(image_root, target_list[t] + '/00000.jpg')\n",
    "        mask_path = os.path.join(mask_root, target_list[t] + '/00000.png')\n",
    "        model_save_path = os.path.join(models_root, target_list[t] + '.pt')\n",
    "    \n",
    "        image = cv2.imread(image_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "        PIL_mask = Image.open(mask_path)\n",
    "        color_to_gray_map, gray_to_color_map = get_map(mask, PIL_mask)\n",
    "        del PIL_mask\n",
    "        \n",
    "        image = cv2.resize(image, Resize, interpolation=cv2.INTER_NEAREST)\n",
    "        mask = cv2.resize(mask, Resize, interpolation=cv2.INTER_NEAREST)\n",
    "        mask = convert_to_gray_mask(mask, color_to_gray_map)\n",
    "        print('type_cnt:', len(color_to_gray_map))\n",
    "    \n",
    "        model = MyResNet(len(color_to_gray_map)).to(device)\n",
    "        train_dataset = CustomDataset(image_path, mask_path, image_transform=train_image_transforms, mask_transform=train_mask_transforms, num_samples=augmentation_num)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "        val_dataset = CustomDataset(image_path, mask_path, image_transform=val_image_transforms, mask_transform=val_mask_transforms, num_samples=1)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "        opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        sch = torch.optim.lr_scheduler.StepLR(opt, step_size=step_size, gamma=gamma)\n",
    "        \n",
    "    \n",
    "        for i in range(train_epoch):\n",
    "            print('epoch:', i)\n",
    "            model.train()\n",
    "            for image, mask in train_dataloader:\n",
    "                mask = (mask * 255).long()\n",
    "                input = torch.cat((image, mask), dim=1).to(device)\n",
    "                output_mask = torch.argmax(model(input), dim=1)\n",
    "                output = model(input)\n",
    "                loss = F.cross_entropy(output, input[:, 3, :, :].long())\n",
    "                # print(loss.item(), torch.sum(output_mask == torch.tensor(input[:, 3, :, :]).to(device)).item() / (224 * 224) / batch_size)\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                sch.step()\n",
    "    \n",
    "            model.eval()\n",
    "            for image, mask in val_dataloader:\n",
    "\n",
    "                mask = (mask * 255).long()\n",
    "                input = torch.cat((image, mask), dim=1).to(device)\n",
    "                output_mask = torch.argmax(model(input), dim=1)\n",
    "                output = model(input)\n",
    "                loss = F.cross_entropy(output, input[:, 3, :, :].long())\n",
    "                print(loss.item(), torch.sum(output_mask == torch.tensor(input[:, 3, :, :]).to(device)).item() / (Resize[0] * Resize[1]))\n",
    "\n",
    "                # mask_np = mask.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "                # output_mask_np = output_mask.cpu().numpy().transpose(1, 2, 0)\n",
    "                # image_np = image.squeeze(0).numpy().transpose(1, 2, 0)\n",
    "                # print_images([image_np, mask_np, np.where(output_mask_np == mask_np, 1, 0)])\n",
    "    \n",
    "        if not os.path.exists(models_root):\n",
    "            os.makedirs(models_root)\n",
    "        torch.save(model.state_dict(), model_save_path)\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T10:01:48.226188Z",
     "start_time": "2024-04-15T09:51:58.092548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train(trainval_image_root, trainval_mask_root, train_list)\n",
    "train(trainval_image_root, trainval_mask_root, val_list)\n",
    "# train(testd_image_root, testd_mask_root, test_list)"
   ],
   "id": "31dcce30045b164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soapbox\n",
      "type_cnt: 4\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stevenzhang/miniconda3/envs/vnn/lib/python3.10/site-packages/segmentation_models_pytorch/base/modules.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.activation(x)\n",
      "/tmp/ipykernel_14779/3707084764.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(loss.item(), torch.sum(output_mask == torch.tensor(input[:, 3, :, :]).to(device)).item() / (Resize[0] * Resize[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2383451461791992 0.8644172512755102\n",
      "epoch: 1\n",
      "0.8579269647598267 0.9835180165816326\n",
      "epoch: 2\n",
      "0.8062392473220825 0.9890186543367347\n",
      "epoch: 3\n",
      "0.9365745186805725 0.803352200255102\n",
      "epoch: 4\n",
      "0.7676920890808105 0.9930644132653061\n",
      "epoch: 5\n",
      "0.7638428211212158 0.9933035714285714\n",
      "epoch: 6\n",
      "0.7599223256111145 0.9935028698979592\n",
      "epoch: 7\n",
      "0.759382963180542 0.9935427295918368\n",
      "epoch: 8\n",
      "0.7566217184066772 0.9939413265306123\n",
      "epoch: 9\n",
      "0.7555509805679321 0.9943598533163265\n",
      "epoch: 10\n",
      "0.7550023794174194 0.9949378188775511\n",
      "epoch: 11\n",
      "0.7540321946144104 0.9948182397959183\n",
      "epoch: 12\n",
      "0.7533095479011536 0.9949577487244898\n",
      "epoch: 13\n",
      "0.7527804374694824 0.9948780293367347\n",
      "epoch: 14\n",
      "0.7523694634437561 0.9950374681122449\n",
      "epoch: 15\n",
      "0.7520291209220886 0.9953563456632653\n",
      "epoch: 16\n",
      "0.7518365979194641 0.9952367665816326\n",
      "epoch: 17\n",
      "0.7514084577560425 0.9954559948979592\n",
      "epoch: 18\n",
      "0.7511677145957947 0.9954161352040817\n",
      "epoch: 19\n",
      "0.7509124875068665 0.9955357142857143\n",
      "epoch: 20\n",
      "0.7509933710098267 0.9955357142857143\n",
      "epoch: 21\n",
      "0.7506046891212463 0.9954958545918368\n",
      "epoch: 22\n",
      "0.7505594491958618 0.9954958545918368\n",
      "epoch: 23\n",
      "0.7504117488861084 0.9953762755102041\n",
      "epoch: 24\n",
      "0.7504306435585022 0.9955157844387755\n",
      "epoch: 25\n",
      "0.7503511309623718 0.9953762755102041\n",
      "epoch: 26\n",
      "0.7501625418663025 0.9956353635204082\n",
      "epoch: 27\n",
      "0.7501479387283325 0.9955556441326531\n",
      "epoch: 28\n",
      "0.7498880624771118 0.9956154336734694\n",
      "epoch: 29\n",
      "0.7497208118438721 0.9957549426020408\n",
      "epoch: 30\n",
      "0.7498002052307129 0.9955955038265306\n",
      "epoch: 31\n",
      "0.7498502135276794 0.9954958545918368\n",
      "epoch: 32\n",
      "0.7496902942657471 0.9955357142857143\n",
      "epoch: 33\n",
      "0.7496094107627869 0.9956353635204082\n",
      "epoch: 34\n",
      "0.7495501041412354 0.9956353635204082\n",
      "epoch: 35\n",
      "0.7495039105415344 0.9956752232142857\n",
      "epoch: 36\n",
      "0.7494083046913147 0.9956552933673469\n",
      "epoch: 37\n",
      "0.7495215535163879 0.9955357142857143\n",
      "epoch: 38\n",
      "0.7494083046913147 0.9955955038265306\n",
      "epoch: 39\n",
      "0.7492344379425049 0.995735012755102\n",
      "epoch: 40\n",
      "0.7494748830795288 0.9955357142857143\n",
      "epoch: 41\n",
      "0.7492730021476746 0.9957150829081632\n",
      "epoch: 42\n",
      "0.7493171691894531 0.9954161352040817\n",
      "epoch: 43\n",
      "0.74936443567276 0.9955157844387755\n",
      "epoch: 44\n",
      "0.7492280006408691 0.9954958545918368\n",
      "epoch: 45\n",
      "0.7491702437400818 0.9957150829081632\n",
      "epoch: 46\n",
      "0.7491780519485474 0.9956752232142857\n",
      "epoch: 47\n",
      "0.7492219805717468 0.9954161352040817\n",
      "epoch: 48\n",
      "0.7491922378540039 0.995475924744898\n",
      "epoch: 49\n",
      "0.7491220235824585 0.9955955038265306\n",
      "epoch: 50\n",
      "0.749082624912262 0.9956951530612245\n",
      "epoch: 51\n",
      "0.749116837978363 0.9954360650510204\n",
      "epoch: 52\n",
      "0.7491596937179565 0.9953364158163265\n",
      "epoch: 53\n",
      "0.7489943504333496 0.9956353635204082\n",
      "epoch: 54\n",
      "0.7489492297172546 0.9957150829081632\n",
      "epoch: 55\n",
      "0.7490499019622803 0.9956752232142857\n",
      "epoch: 56\n",
      "0.7489697933197021 0.9954559948979592\n",
      "epoch: 57\n",
      "0.749133825302124 0.9956154336734694\n",
      "epoch: 58\n",
      "0.7489619851112366 0.9957150829081632\n",
      "epoch: 59\n",
      "0.7489058971405029 0.9956353635204082\n",
      "epoch: 60\n",
      "0.7489429116249084 0.9956353635204082\n",
      "epoch: 61\n",
      "0.7488347291946411 0.9956154336734694\n",
      "epoch: 62\n",
      "0.7489644885063171 0.9955955038265306\n",
      "epoch: 63\n",
      "0.7488275766372681 0.995735012755102\n",
      "epoch: 64\n",
      "0.7489538192749023 0.9956353635204082\n",
      "epoch: 65\n",
      "0.748914897441864 0.9956353635204082\n",
      "epoch: 66\n",
      "0.7488718032836914 0.9956552933673469\n",
      "epoch: 67\n",
      "0.7488287091255188 0.995735012755102\n",
      "epoch: 68\n",
      "0.7489578723907471 0.9956154336734694\n",
      "epoch: 69\n",
      "0.7488389015197754 0.9956752232142857\n",
      "epoch: 70\n",
      "0.7488481998443604 0.9956154336734694\n",
      "epoch: 71\n",
      "0.7488746047019958 0.9956752232142857\n",
      "epoch: 72\n",
      "0.7488365769386292 0.9957150829081632\n",
      "epoch: 73\n",
      "0.7490763664245605 0.9956154336734694\n",
      "epoch: 74\n",
      "0.748848557472229 0.9956154336734694\n",
      "epoch: 75\n",
      "0.748792827129364 0.9957150829081632\n",
      "epoch: 76\n",
      "0.7487854957580566 0.9956353635204082\n",
      "epoch: 77\n",
      "0.7488136887550354 0.995735012755102\n",
      "epoch: 78\n",
      "0.7487746477127075 0.995735012755102\n",
      "epoch: 79\n",
      "0.7488837838172913 0.9956951530612245\n",
      "epoch: 80\n",
      "0.7487884759902954 0.9956951530612245\n",
      "epoch: 81\n",
      "0.7487673163414001 0.9957150829081632\n",
      "epoch: 82\n",
      "0.7488433718681335 0.9956154336734694\n",
      "epoch: 83\n",
      "0.7487612366676331 0.9956951530612245\n",
      "epoch: 84\n",
      "0.7486996054649353 0.9957549426020408\n",
      "epoch: 85\n",
      "0.7487450242042542 0.9957150829081632\n",
      "epoch: 86\n",
      "0.7487632632255554 0.9956951530612245\n",
      "epoch: 87\n",
      "0.748779833316803 0.9957150829081632\n",
      "epoch: 88\n",
      "0.748737096786499 0.9957150829081632\n",
      "epoch: 89\n",
      "0.7487908005714417 0.9956951530612245\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs5340)",
   "language": "python",
   "name": "cs5340"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
